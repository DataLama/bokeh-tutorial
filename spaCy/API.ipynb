{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc object owns the sequence of tokens and all their annotations.\n",
    "- Vocab object owns a set of look-up tables that make common information available across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy + custom tokenizer... + basic tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.lang.ko import Korean, KoreanTokenizer\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "nlp = Korean()\n",
    "tokenizer = LTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14802192752119398744"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[13].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전 DET\n",
      "건성 NOUN\n",
      "입니다 ADP\n",
      "😊 SYM\n",
      "적당 X\n",
      "한 X\n",
      "유분 NOUN\n",
      "도 ADP\n",
      "가지 AUX\n",
      "고 X\n",
      "있 AUX\n",
      "는 X\n",
      "데 NOUN\n",
      "가벼워서 ADJ\n",
      "화장 NOUN\n",
      "전 NOUN\n",
      "에 ADP\n",
      "바르 VERB\n",
      "기 X\n",
      "도 ADP\n",
      "좋 ADJ\n",
      "아요 X\n",
      "! PUNCT\n",
      "밤 NOUN\n",
      "에 ADP\n",
      "바르 VERB\n",
      "면 X\n",
      "속 NOUN\n",
      "까지 ADP\n",
      "유수 NOUN\n",
      "분감 NOUN\n",
      "이 ADP\n",
      "차오르 VERB\n",
      "고 X\n",
      "촉촉 X\n",
      "하 X\n",
      "고 X\n",
      "전 PRON\n",
      "아주 ADV\n",
      "좋 ADJ\n",
      "아요 X\n",
      "악 X\n",
      "건성 NOUN\n",
      "인 ADP\n",
      "분 NOUN\n",
      "들 X\n",
      "은 ADP\n",
      "크림 NOUN\n",
      "하나 NUM\n",
      "더 ADV\n",
      "발라야 VERB\n",
      "할 AUX\n",
      "듯 NOUN\n",
      "싶 AUX\n",
      "어요 X\n",
      "! PUNCT\n",
      "전 PRON\n",
      "엄청난 ADJ\n",
      "건성 NOUN\n",
      "은 ADP\n",
      "아니 ADJ\n",
      "라서 X\n",
      "피곤 NOUN\n",
      "할 X\n",
      "때 NOUN\n",
      "는 ADP\n",
      "그냥 ADV\n",
      "로션 NOUN\n",
      "만 ADP\n",
      "바르 VERB\n",
      "기 X\n",
      "도 ADP\n",
      "해요 VERB\n",
      "! PUNCT\n",
      "아침 NOUN\n",
      "에 ADP\n",
      "는 ADP\n",
      "스킨 NOUN\n",
      "바르 VERB\n",
      "고 X\n",
      "로션 NOUN\n",
      "만 ADP\n",
      "바르 ADJ\n",
      "고 X\n",
      "선크림 NOUN\n",
      "바르 VERB\n",
      "고 X\n",
      "화장 NOUN\n",
      "해요 X\n",
      "! PUNCT\n",
      "그래도 CONJ\n",
      "적당히 ADV\n",
      "촉촉 X\n",
      "하 X\n",
      "고 X\n",
      "좋 ADJ\n",
      "아요 X\n",
      "화장 NOUN\n",
      "전 NOUN\n",
      "기초 NOUN\n",
      "를 ADP\n",
      "여러 DET\n",
      "개 NOUN\n",
      "해서 VERB\n",
      "피부 NOUN\n",
      "답답 X\n",
      "한 X\n",
      "거 NOUN\n",
      "싫 ADJ\n",
      "은데 X\n",
      "이 DET\n",
      "로션 NOUN\n",
      "은 ADP\n",
      "안 ADV\n",
      "그래서 VERB\n",
      "좋 ADJ\n",
      "아요 X\n",
      "이미 ADV\n",
      "한 DET\n",
      "통 NOUN\n",
      "비워서 VERB\n",
      "한 DET\n",
      "통 NOUN\n",
      "더 ADV\n",
      "살 VERB\n",
      "생각 NOUN\n",
      "입니다 ADP\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoyTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = tokenizer.tokenize(text)\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = SoyTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"전 건성입니다😊\n",
    "적당한 유분도 가지고 있는데 가벼워서 화장 전에 바르기도 좋아요!\n",
    "밤에 바르면 속까지 유수분감이 차오르고 촉촉하고 전 아주 좋아요 악건성인 분들은 크림 하나 더 발라야 할 듯 싶어요! 전 엄청난 건성은 아니라서 피곤할 때는 그냥 로션만 바르기도 해요!\n",
    "아침에는 스킨 바르고 로션만 바르고 선크림 바르고 화장해요! 그래도 적당히 촉촉하고 좋아요 화장 전 기초를 여러개 해서 피부 답답한 거 싫은데 이 로션은 안 그래서 좋아요 이미 한 통 비워서 한 통 더 살 생각입니다!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전 \n",
      "건성입니다😊 \n",
      "적당한 \n",
      "유분도 \n",
      "가지고 \n",
      "있는데 \n",
      "가벼워서 \n",
      "화장 \n",
      "전에 \n",
      "바르기도 \n",
      "좋아요! \n",
      "밤에 \n",
      "바르면 \n",
      "속까지 \n",
      "유수분감이 \n",
      "차오르고 \n",
      "촉촉하고 \n",
      "전 \n",
      "아주 \n",
      "좋아요 \n",
      "악건성인 \n",
      "분들은 \n",
      "크림 \n",
      "하나 \n",
      "더 \n",
      "발라야 \n",
      "할 \n",
      "듯 \n",
      "싶어요! \n",
      "전 \n",
      "엄청난 \n",
      "건성은 \n",
      "아니라서 \n",
      "피곤할 \n",
      "때는 \n",
      "그냥 \n",
      "로션만 \n",
      "바르기도 \n",
      "해요! \n",
      "아침에는 \n",
      "스킨 \n",
      "바르고 \n",
      "로션만 \n",
      "바르고 \n",
      "선크림 \n",
      "바르고 \n",
      "화장해요! \n",
      "그래도 \n",
      "적당히 \n",
      "촉촉하고 \n",
      "좋아요 \n",
      "화장 \n",
      "전 \n",
      "기초를 \n",
      "여러개 \n",
      "해서 \n",
      "피부 \n",
      "답답한 \n",
      "거 \n",
      "싫은데 \n",
      "이 \n",
      "로션은 \n",
      "안 \n",
      "그래서 \n",
      "좋아요 \n",
      "이미 \n",
      "한 \n",
      "통 \n",
      "비워서 \n",
      "한 \n",
      "통 \n",
      "더 \n",
      "살 \n",
      "생각입니다! \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14630057439605637705"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'바르면'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[12]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[12].is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[doc[1].text].is_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'가'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].prefix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'벼워서'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].suffix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'check_flag',\n",
       " 'cluster',\n",
       " 'flags',\n",
       " 'from_bytes',\n",
       " 'has_vector',\n",
       " 'is_alpha',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'sentiment',\n",
       " 'set_attrs',\n",
       " 'set_flag',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'text',\n",
       " 'to_bytes',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlp.vocab[doc[0].text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['전', '건성입니다😊', '적당한', '유분도', '가지고', '있는데', '가벼워서', '화장', '전에', '바르기도', '좋아요!', '밤에', '바르면', '속까지', '유수분감이', '차오르고', '촉촉하고', '전', '아주', '좋아요', '악건성인', '분들은', '크림', '하나', '더', '발라야', '할', '듯', '싶어요!', '전', '엄청난', '건성은', '아니라서', '피곤할', '때는', '그냥', '로션만', '바르기도', '해요!', '아침에는', '스킨', '바르고', '로션만', '바르고', '선크림', '바르고', '화장해요!', '그래도', '적당히', '촉촉하고', '좋아요', '화장', '전', '기초를', '여러개', '해서', '피부', '답답한', '거', '싫은데', '이', '로션은', '안', '그래서', '좋아요', '이미', '한', '통', '비워서', '한', '통', '더', '살', '생각입니다!']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Korean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SYM'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전\n",
      "건성입니다😊\n",
      "적당한\n",
      "유분도\n",
      "가지고\n",
      "있는데\n",
      "가벼워서\n",
      "화장\n",
      "전에\n",
      "바르기도\n",
      "좋아요!\n",
      "밤에\n",
      "바르면\n",
      "속까지\n",
      "유수분감이\n",
      "차오르고\n",
      "촉촉하고\n",
      "전\n",
      "아주\n",
      "좋아요\n",
      "악건성인\n",
      "분들은\n",
      "크림\n",
      "하나\n",
      "더\n",
      "발라야\n",
      "할\n",
      "듯\n",
      "싶어요!\n",
      "전\n",
      "엄청난\n",
      "건성은\n",
      "아니라서\n",
      "피곤할\n",
      "때는\n",
      "그냥\n",
      "로션만\n",
      "바르기도\n",
      "해요!\n",
      "아침에는\n",
      "스킨\n",
      "바르고\n",
      "로션만\n",
      "바르고\n",
      "선크림\n",
      "바르고\n",
      "화장해요!\n",
      "그래도\n",
      "적당히\n",
      "촉촉하고\n",
      "좋아요\n",
      "화장\n",
      "전\n",
      "기초를\n",
      "여러개\n",
      "해서\n",
      "피부\n",
      "답답한\n",
      "거\n",
      "싫은데\n",
      "이\n",
      "로션은\n",
      "안\n",
      "그래서\n",
      "좋아요\n",
      "이미\n",
      "한\n",
      "통\n",
      "비워서\n",
      "한\n",
      "통\n",
      "더\n",
      "살\n",
      "생각입니다!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['전',\n",
       " '건성입니다😊',\n",
       " '적당한',\n",
       " '유분도',\n",
       " '가지고',\n",
       " '있는데',\n",
       " '가벼워서',\n",
       " '화장',\n",
       " '전에',\n",
       " '바르기도',\n",
       " '좋아요!',\n",
       " '밤에',\n",
       " '바르면',\n",
       " '속까지',\n",
       " '유수분감이',\n",
       " '차오르고',\n",
       " '촉촉하고',\n",
       " '전',\n",
       " '아주',\n",
       " '좋아요',\n",
       " '악건성인',\n",
       " '분들은',\n",
       " '크림',\n",
       " '하나',\n",
       " '더',\n",
       " '발라야',\n",
       " '할',\n",
       " '듯',\n",
       " '싶어요!',\n",
       " '전',\n",
       " '엄청난',\n",
       " '건성은',\n",
       " '아니라서',\n",
       " '피곤할',\n",
       " '때는',\n",
       " '그냥',\n",
       " '로션만',\n",
       " '바르기도',\n",
       " '해요!',\n",
       " '아침에는',\n",
       " '스킨',\n",
       " '바르고',\n",
       " '로션만',\n",
       " '바르고',\n",
       " '선크림',\n",
       " '바르고',\n",
       " '화장해요!',\n",
       " '그래도',\n",
       " '적당히',\n",
       " '촉촉하고',\n",
       " '좋아요',\n",
       " '화장',\n",
       " '전',\n",
       " '기초를',\n",
       " '여러개',\n",
       " '해서',\n",
       " '피부',\n",
       " '답답한',\n",
       " '거',\n",
       " '싫은데',\n",
       " '이',\n",
       " '로션은',\n",
       " '안',\n",
       " '그래서',\n",
       " '좋아요',\n",
       " '이미',\n",
       " '한',\n",
       " '통',\n",
       " '비워서',\n",
       " '한',\n",
       " '통',\n",
       " '더',\n",
       " '살',\n",
       " '생각입니다!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라서\n",
      "7\n",
      "비워서\n",
      "만\n",
      "빠른\n",
      "해요\n",
      "아침\n",
      "피곤\n",
      "보였\n",
      "생각\n",
      "할\n",
      "세트\n",
      "인\n",
      "분감\n",
      "유수\n",
      "폼\n",
      "이거\n",
      "같\n",
      "건성\n",
      "도\n",
      "유분\n",
      "통\n",
      "습니다\n",
      "발라야\n",
      "은\n",
      "트러블\n",
      "살\n",
      "적당히\n",
      "고\n",
      "화장\n",
      "때문\n",
      "아주\n",
      "어요\n",
      "그래도\n",
      "셨\n",
      "다\n",
      "이미\n",
      "바르\n",
      "더\n",
      "한\n",
      "는\n",
      "꽤\n",
      "이랑\n",
      "가벼워서\n",
      "치료\n",
      "있\n",
      "아요\n",
      "이걸\n",
      "하나\n",
      "싫\n",
      "여러\n",
      "!\n",
      "데\n",
      "그래서\n",
      "😊\n",
      "가지\n",
      "전\n",
      "입\n",
      "니당\n",
      "거\n",
      "엄청난\n",
      "까지\n",
      "는데\n",
      "산\n",
      "답답\n",
      "추천\n",
      "에서\n",
      "속\n",
      "악\n",
      "해\n",
      "던\n",
      "제품\n",
      "로션\n",
      "선크림\n",
      "피부과\n",
      "좁쌀\n",
      "해용\n",
      "화농성\n",
      "싶\n",
      "분\n",
      "적당\n",
      "면\n",
      "사\n",
      "안\n",
      "주\n",
      "게\n",
      "갔었\n",
      "었\n",
      "로\n",
      "촉촉\n",
      "되\n",
      "를\n",
      "개\n",
      "시간\n",
      "기초\n",
      "클렌징\n",
      "것\n",
      "에\n",
      "이것저것\n",
      "듯\n",
      "해서\n",
      "이\n",
      "크림\n",
      "은데\n",
      "들\n",
      "원\n",
      "산뜻\n",
      "피부\n",
      "하\n",
      "밤\n",
      "그냥\n",
      "스킨\n",
      "차오르\n",
      "정도\n",
      "때\n",
      "줄어드\n",
      "입니다\n",
      "기\n",
      "아니\n",
      "좋\n"
     ]
    }
   ],
   "source": [
    "for v in nlp.vocab:\n",
    "    print(v.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:40:15,747\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-25 01:40:15,751\tINFO resource_spec.py:216 -- Starting Ray with 297.56 GiB memory available for workers and up to 195.31 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.init(num_cpus=64, object_store_memory = 200000 * 1024 * 1024, driver_object_store_memory = 100000 * 1024 * 1024)\n",
    "\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/social_buzz_dataset/korean_dataset_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   ['김슬기이소미', '와', '종나이뻐']\n",
       "1                            ['아까', '피곤할때', '닡', '눈', '저럼']\n",
       "2                        ['처음', '봐봐', '걍', '니야', '눈빛', '도']\n",
       "3         ['이희재', '아니', '희재야', '에반데', '너', 'ㅇ거', '진짜', '...\n",
       "4                                          ['성민아설리', '지린다']\n",
       "                                ...                        \n",
       "455055    ['구정', '특가', '에스티로더', '리바이탈라이징', '수프림', '글로', ...\n",
       "455056    ['영어', '안쓰고', '한국', '어만', '들어간', '영상', '도', '찍...\n",
       "455057    ['원칙', '인스타', '거른다', '어플', '앱', '등', '거른다', '유...\n",
       "455058                                        ['감사', '합니다']\n",
       "455059                                       ['lets', 'go']\n",
       "Name: Comments_tokens, Length: 455060, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comments_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Media Type</th>\n",
       "      <th>Site Name</th>\n",
       "      <th>Site Domain</th>\n",
       "      <th>Mention URL</th>\n",
       "      <th>Publisher Name</th>\n",
       "      <th>Publisher Username</th>\n",
       "      <th>Mention Title</th>\n",
       "      <th>...</th>\n",
       "      <th>Youtube Favorites</th>\n",
       "      <th>Synthesio Rank</th>\n",
       "      <th>User Age</th>\n",
       "      <th>User Gender</th>\n",
       "      <th>User Family Status</th>\n",
       "      <th>User Marital Status</th>\n",
       "      <th>User Affinities</th>\n",
       "      <th>User Jobs</th>\n",
       "      <th>User biography tags</th>\n",
       "      <th>Comments_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360101-91195377833</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>23:59:00 +0900 KST</td>\n",
       "      <td>Social network</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anonymous user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estee Lauder KR 에스티 로더</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['김슬기이소미', '와', '종나이뻐']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360101-91195377777</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>23:59:00 +0900 KST</td>\n",
       "      <td>Social network</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anonymous user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estee Lauder KR 에스티 로더</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['아까', '피곤할때', '닡', '눈', '저럼']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360101-91195377678</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>23:58:00 +0900 KST</td>\n",
       "      <td>Social network</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anonymous user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estee Lauder KR 에스티 로더</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['처음', '봐봐', '걍', '니야', '눈빛', '도']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360101-91195377621</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>23:58:00 +0900 KST</td>\n",
       "      <td>Social network</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anonymous user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estee Lauder KR 에스티 로더</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['이희재', '아니', '희재야', '에반데', '너', 'ㅇ거', '진짜', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360101-91195377351</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>23:57:00 +0900 KST</td>\n",
       "      <td>Social network</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>http://www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anonymous user</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estee Lauder KR 에스티 로더</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['성민아설리', '지린다']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455055</th>\n",
       "      <td>360101-138651752690</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>00:21:00 +0900 KST</td>\n",
       "      <td>Forum</td>\n",
       "      <td>Cafe.Naver</td>\n",
       "      <td>http://cafe.naver.com</td>\n",
       "      <td>https://cafe.naver.com/ArticleRead.nhn?clubid=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[뉴저지콕콕] 구정특가* 에스티로더 리바이탈라이징 수프림+ 글로벌 안티 에이징 파워...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['구정', '특가', '에스티로더', '리바이탈라이징', '수프림', '글로', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455056</th>\n",
       "      <td>360101-138585747518</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>00:04:00 +0900 KST</td>\n",
       "      <td>Video and Photo Sharing</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>http://www.youtube.com</td>\n",
       "      <td>https://www.youtube.com/watch?v=tv0xwv1vsfU&amp;lc...</td>\n",
       "      <td>빈은</td>\n",
       "      <td>UCklZYDy21MopvNiR4JvDAZA</td>\n",
       "      <td>Full Face in 10 Min ⚡ ASMR: CHANEL, Jeffree St...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['영어', '안쓰고', '한국', '어만', '들어간', '영상', '도', '찍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455057</th>\n",
       "      <td>360101-138569083547</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>00:04:00 +0900 KST</td>\n",
       "      <td>Forum</td>\n",
       "      <td>DC Inside</td>\n",
       "      <td>http://www.dcinside.com</td>\n",
       "      <td>https://gall.dcinside.com/board/view/?id=cosme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기초 화장품 추천</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['원칙', '인스타', '거른다', '어플', '앱', '등', '거른다', '유...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455058</th>\n",
       "      <td>360101-138601418195</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>00:02:00 +0900 KST</td>\n",
       "      <td>Forum</td>\n",
       "      <td>Cafe.Naver</td>\n",
       "      <td>http://cafe.naver.com</td>\n",
       "      <td>https://cafe.naver.com/ArticleRead.nhn?clubid=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[이천팩토리] 설화수 자음2종백화점 라인 구하기힘든제품 (선물강추) 71000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['감사', '합니다']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455059</th>\n",
       "      <td>360101-138585747588</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>00:01:00 +0900 KST</td>\n",
       "      <td>Video and Photo Sharing</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>http://www.youtube.com</td>\n",
       "      <td>https://www.youtube.com/watch?v=tv0xwv1vsfU&amp;lc...</td>\n",
       "      <td>Fayza Aurora</td>\n",
       "      <td>UCb9Q4Lff_2mJz20DQuEk78w</td>\n",
       "      <td>Full Face in 10 Min ⚡ ASMR: CHANEL, Jeffree St...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['lets', 'go']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455060 rows x 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Id        Date                Time  \\\n",
       "0        360101-91195377833  2017-12-30  23:59:00 +0900 KST   \n",
       "1        360101-91195377777  2017-12-30  23:59:00 +0900 KST   \n",
       "2        360101-91195377678  2017-12-30  23:58:00 +0900 KST   \n",
       "3        360101-91195377621  2017-12-30  23:58:00 +0900 KST   \n",
       "4        360101-91195377351  2017-12-30  23:57:00 +0900 KST   \n",
       "...                     ...         ...                 ...   \n",
       "455055  360101-138651752690  2020-01-24  00:21:00 +0900 KST   \n",
       "455056  360101-138585747518  2020-01-24  00:04:00 +0900 KST   \n",
       "455057  360101-138569083547  2020-01-24  00:04:00 +0900 KST   \n",
       "455058  360101-138601418195  2020-01-24  00:02:00 +0900 KST   \n",
       "455059  360101-138585747588  2020-01-24  00:01:00 +0900 KST   \n",
       "\n",
       "                     Media Type   Site Name              Site Domain  \\\n",
       "0                Social network    Facebook  http://www.facebook.com   \n",
       "1                Social network    Facebook  http://www.facebook.com   \n",
       "2                Social network    Facebook  http://www.facebook.com   \n",
       "3                Social network    Facebook  http://www.facebook.com   \n",
       "4                Social network    Facebook  http://www.facebook.com   \n",
       "...                         ...         ...                      ...   \n",
       "455055                    Forum  Cafe.Naver    http://cafe.naver.com   \n",
       "455056  Video and Photo Sharing     YouTube   http://www.youtube.com   \n",
       "455057                    Forum   DC Inside  http://www.dcinside.com   \n",
       "455058                    Forum  Cafe.Naver    http://cafe.naver.com   \n",
       "455059  Video and Photo Sharing     YouTube   http://www.youtube.com   \n",
       "\n",
       "                                              Mention URL  Publisher Name  \\\n",
       "0                                                     NaN  Anonymous user   \n",
       "1                                                     NaN  Anonymous user   \n",
       "2                                                     NaN  Anonymous user   \n",
       "3                                                     NaN  Anonymous user   \n",
       "4                                                     NaN  Anonymous user   \n",
       "...                                                   ...             ...   \n",
       "455055  https://cafe.naver.com/ArticleRead.nhn?clubid=...             NaN   \n",
       "455056  https://www.youtube.com/watch?v=tv0xwv1vsfU&lc...              빈은   \n",
       "455057  https://gall.dcinside.com/board/view/?id=cosme...             NaN   \n",
       "455058  https://cafe.naver.com/ArticleRead.nhn?clubid=...             NaN   \n",
       "455059  https://www.youtube.com/watch?v=tv0xwv1vsfU&lc...    Fayza Aurora   \n",
       "\n",
       "              Publisher Username  \\\n",
       "0                            NaN   \n",
       "1                            NaN   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "...                          ...   \n",
       "455055                       NaN   \n",
       "455056  UCklZYDy21MopvNiR4JvDAZA   \n",
       "455057                       NaN   \n",
       "455058                       NaN   \n",
       "455059  UCb9Q4Lff_2mJz20DQuEk78w   \n",
       "\n",
       "                                            Mention Title  ...  \\\n",
       "0                                  Estee Lauder KR 에스티 로더  ...   \n",
       "1                                  Estee Lauder KR 에스티 로더  ...   \n",
       "2                                  Estee Lauder KR 에스티 로더  ...   \n",
       "3                                  Estee Lauder KR 에스티 로더  ...   \n",
       "4                                  Estee Lauder KR 에스티 로더  ...   \n",
       "...                                                   ...  ...   \n",
       "455055  [뉴저지콕콕] 구정특가* 에스티로더 리바이탈라이징 수프림+ 글로벌 안티 에이징 파워...  ...   \n",
       "455056  Full Face in 10 Min ⚡ ASMR: CHANEL, Jeffree St...  ...   \n",
       "455057                                          기초 화장품 추천  ...   \n",
       "455058        [이천팩토리] 설화수 자음2종백화점 라인 구하기힘든제품 (선물강추) 71000  ...   \n",
       "455059  Full Face in 10 Min ⚡ ASMR: CHANEL, Jeffree St...  ...   \n",
       "\n",
       "       Youtube Favorites  Synthesio Rank  User Age  User Gender  \\\n",
       "0                    NaN             1.6       NaN          NaN   \n",
       "1                    NaN             1.6       NaN          NaN   \n",
       "2                    NaN             1.6       NaN          NaN   \n",
       "3                    NaN             1.6       NaN          NaN   \n",
       "4                    NaN             1.6       NaN          NaN   \n",
       "...                  ...             ...       ...          ...   \n",
       "455055               NaN             5.0       NaN          NaN   \n",
       "455056               NaN             5.0       NaN          NaN   \n",
       "455057               NaN             5.0       NaN          NaN   \n",
       "455058               NaN             5.0       NaN          NaN   \n",
       "455059               NaN             5.0       NaN          NaN   \n",
       "\n",
       "        User Family Status  User Marital Status  User Affinities  User Jobs  \\\n",
       "0                      NaN                  NaN              NaN        NaN   \n",
       "1                      NaN                  NaN              NaN        NaN   \n",
       "2                      NaN                  NaN              NaN        NaN   \n",
       "3                      NaN                  NaN              NaN        NaN   \n",
       "4                      NaN                  NaN              NaN        NaN   \n",
       "...                    ...                  ...              ...        ...   \n",
       "455055                 NaN                  NaN              NaN        NaN   \n",
       "455056                 NaN                  NaN              NaN        NaN   \n",
       "455057                 NaN                  NaN              NaN        NaN   \n",
       "455058                 NaN                  NaN              NaN        NaN   \n",
       "455059                 NaN                  NaN              NaN        NaN   \n",
       "\n",
       "        User biography tags                                    Comments_tokens  \n",
       "0                       NaN                            ['김슬기이소미', '와', '종나이뻐']  \n",
       "1                       NaN                     ['아까', '피곤할때', '닡', '눈', '저럼']  \n",
       "2                       NaN                 ['처음', '봐봐', '걍', '니야', '눈빛', '도']  \n",
       "3                       NaN  ['이희재', '아니', '희재야', '에반데', '너', 'ㅇ거', '진짜', '...  \n",
       "4                       NaN                                   ['성민아설리', '지린다']  \n",
       "...                     ...                                                ...  \n",
       "455055                  NaN  ['구정', '특가', '에스티로더', '리바이탈라이징', '수프림', '글로', ...  \n",
       "455056                  NaN  ['영어', '안쓰고', '한국', '어만', '들어간', '영상', '도', '찍...  \n",
       "455057                  NaN  ['원칙', '인스타', '거른다', '어플', '앱', '등', '거른다', '유...  \n",
       "455058                  NaN                                      ['감사', '합니다']  \n",
       "455059                  NaN                                     ['lets', 'go']  \n",
       "\n",
       "[455060 rows x 52 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- id, comment_text...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lexeme == tokenizer...\n",
    "- keyword == whitespace 단위 + 쓸데 없는 토큰 제거..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinese(use_jieba = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Jieba not installed. Either set Chinese.use_jieba = False, or install it https://github.com/fxsjy/jieba",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mtry_jieba_import\u001b[0;34m(use_jieba)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-f317f9299d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChinese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_jieba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"蘋果公司正考量用一億元買下英國的新創公司\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, make_doc, max_length, meta, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmake_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[0;34m(cls, nlp)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mChineseTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cls, nlp)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_jieba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_jieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjieba_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_jieba_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_jieba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mtry_jieba_import\u001b[0;34m(use_jieba)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34m\"or install it https://github.com/fxsjy/jieba\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             )\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Jieba not installed. Either set Chinese.use_jieba = False, or install it https://github.com/fxsjy/jieba"
     ]
    }
   ],
   "source": [
    "nlp = \n",
    "doc2 = nlp(u\"蘋果公司正考量用一億元買下英國的新創公司\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.language.Language.<lambda>(nlp)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chinese.factories['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function <lambda> in module spacy.language:\n",
      "\n",
      "<lambda> lambda nlp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Chinese)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
