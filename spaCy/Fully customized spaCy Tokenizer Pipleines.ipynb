{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully customized spaCy Tokenizer Pipleines\n",
    "- spaCyì˜ nlpì— ëŒ€í•˜ì—¬ ê°€ì¥ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ tokenizationì´ë‹¤.\n",
    "- spaCyëŠ” ê·œì¹™ ê¸°ë°˜ì˜ tokenization ë°©ì‹ì„ ì§€ì›í•˜ì§€ë§Œ, ê¸°ë³¸ì ìœ¼ë¡œ ë¼í‹´ì–´ì— ì í•©í•œ í˜•íƒœì„.\n",
    "- ê·¸ëŸ¬ë¯€ë¡œ, ì•„ì‹œì•„ê¶Œ ì–¸ì–´ì— ëŒ€í•´ì„œëŠ” ê° êµ­ê°€ë³„ë¡œ ì´ë¯¸ ë§Œë“¤ì–´ì§„ tokenizerë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp pipelineì— í† í¬ë‚˜ì´ì € wrappingí•˜ê¸°.\n",
    "- spacy í•œêµ­ì–´ì˜ ê²½ìš° mecabê³¼ nattoë¥¼ ë°±ë³¸ìœ¼ë¡œ ë§Œë“¤ì–´ì§.\n",
    "- Domain-specificí•˜ê²Œ í•™ìŠµí•œ soynlp tokenizerë¡œ spacy tokenizerë¥¼ wrappingí•  ê²ƒì„."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ì •ì˜í•˜ê¸°.\n",
    "#### callable objectë¡œ ë§Œë“¤ì.\n",
    "- input : text\n",
    "- output : Doc object\n",
    "\n",
    "#### `nlp.vocab`ì„ ë„˜ê²¨ì£¼ì.\n",
    "- ì–¸ì–´ë³„ ê¸°ë³¸ nlpë¥¼ ë„˜ê²¨ì£¼ì.\n",
    "\n",
    "#### custom variable\n",
    "- í˜•íƒœì†Œ ë¶„ì„ê¸°ë°˜ í† í¬ë‚˜ì´ì €\n",
    "    - spacytokenizer ì»¤ìŠ¤í…€\n",
    "- ì»¤ìŠ¤í…€ í’ˆì‚¬ ì •ì˜\n",
    "    - tokenizer ì»¤ìŠ¤í…€\n",
    "    - spacytokenizer ì»¤ìŠ¤í…€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜ˆì‹œ\n",
    "- soynlp ê°™ì€ ê²½ìš° í’ˆì‚¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í›„ì²˜ë¦¬ë¡œ Lê³¼ Rì„ ì²˜ë¦¬í•˜ëŠ” ê°ì²´ ì •ì˜í•¨. (`pipeline/ko.py`í™•ì¸)\n",
    "- SpacyTokenizerì— ê¸° í’ˆì‚¬ ë˜ëŠ” ì»¤ìŠ¤í…€ í’ˆì‚¬ë¥¼ attributeë¡œ ë„˜ê²¨ë°›ê¸° ìœ„í•´ì„œ Attribute extensions(`._.something`)ì„ í™œìš©í•¨.\n",
    "    - ì˜¤í”¼ì…œí•˜ê²ŒëŠ” tag attributeì— wrappingí•˜ëŠ” ê²ƒì´ ë§ëŠ” ë°©ë²•ì´ë‚˜, ê°„ë‹¨í•˜ê²Œ attribute extensionsì„ í™œìš©í•˜ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define custom attribute globally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension(\"tag_\", default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define and initialize tokenizer and SpacyTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ko import Korean\n",
    "from pipeline import ko\n",
    "\n",
    "nlp = Korean()\n",
    "nlp.tokenizer = ko.SpacyTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìš”ìƒˆ L\n",
      "ì½”ë¡œë‚˜ L\n",
      "ë•Œë¬¸ L\n",
      "ì— R\n",
      "ë§ˆìŠ¤í¬ L\n",
      "ë¥¼ R\n",
      "ë§ì´ L\n",
      "ì“°ëŠ” L\n",
      "ì‚¬ëŒë“¤ L\n",
      "ì€ R\n",
      "í”¼ë¶€ L\n",
      "ê°€ R\n",
      "ë” L\n",
      "ê±°ì¹ ì–´ì§€ê³  L\n",
      "ê±´ì¡° L\n",
      "í•˜ê³  R\n",
      "ìê·¹ L\n",
      "ì„ R\n",
      "ë§ì´ë°›ì•„ì„œ L\n",
      "í”¼ë¶€ L\n",
      "íŠ¸ëŸ¬ë¸” L\n",
      "ë„ R\n",
      "ë§ì•„ L\n",
      "ì§€ëŠ” L\n",
      "ì‹œì  L\n",
      "ì´êµ¬ìš” R\n"
     ]
    }
   ],
   "source": [
    "for token in nlp('ìš”ìƒˆ ì½”ë¡œë‚˜ ë•Œë¬¸ì— ë§ˆìŠ¤í¬ë¥¼ ë§ì´ ì“°ëŠ” ì‚¬ëŒë“¤ì€ í”¼ë¶€ê°€ ë” ê±°ì¹ ì–´ì§€ê³  ê±´ì¡°í•˜ê³  ìê·¹ì„ ë§ì´ë°›ì•„ì„œ í”¼ë¶€ íŠ¸ëŸ¬ë¸”ë„ ë§ì•„ ì§€ëŠ” ì‹œì ì´êµ¬ìš”!'):\n",
    "    print(token.text, token._.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp pipline with text in tabular data\n",
    "- tabular dataì— í¬í•¨ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì˜ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°í”„ë ˆì„ê³¼ spacy object ê°„ì˜ ììœ ë¡œìš´ transformì´ ê°€ëŠ¥í•´ì•¼ëœë‹¤.\n",
    "- **df $\\longrightarrow$ spacy $\\longrightarrow$ df**ì™€ ê°™ì€ ì¼ë ¨ì˜ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from `df` to `spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tabular ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , spacy ë³€ìˆ˜ ì •ì˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Mention Title</th>\n",
       "      <th>Mention Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360101-102055396016</td>\n",
       "      <td>Sulwhasoo</td>\n",
       "      <td>ì •ë‹µ : ì‰¬ì–´ ë˜ìŠ¤íŒ… ì´ì•„ë¦„ ë‹˜ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360101-141133562509</td>\n",
       "      <td>[ë‘ì½¤] ìˆ˜ì§€ê°€ ì†Œê°œí•˜ëŠ” ë¦¬ì–¼ ë©”ì´í¬ì—… íŒ! Suzy's Real Make-up t...</td>\n",
       "      <td>ì²œì‚¬ ìˆ˜ì§€â¤ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360101-92181315780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#ì—ìŠ¤í‹°ë¡œë” 39ì»¬ëŸ¬ NEW ì‹ ìƒë¦½! #ì—”ë¹„ë¦½í˜ì¸íŠ¸ ì§€ì†ë ¥ì´ ì„ë§¤ë‚˜ ì§±ì§±í—Œì§€,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360101-133213819477</td>\n",
       "      <td>ê¹€ì„ ì•„ ê³µí•­íŒ¨ì…˜ ìŠ¤íƒ€ì¼ë§ ê¸€ì“´ì´ ë³´ë°° ë“±ë¡ì¼ 2019-11-04 15:24 ì¡°íšŒìˆ˜ ...</td>\n",
       "      <td>ì „ì²´ë³´ê¸° ì¼ìƒ ë¶€ë¶€ ì‹œì§‘/ì¹œì • ìœ¡ì•„ ì§ì¥ì‚´ë¦¼ ì¢‹ì€ê¸€ ì›ƒìŒ ê³ ë¯¼/ìµëª… ì—°ì˜ˆ/ì‹œì‚¬ ë‚´ê°€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360101-74466121153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#ë‘ì½¤#ë¦½ìŠ¤í‹±#ë°•ì ¼ë‹˜ì„ ë¬¼ğŸŒºê³ ë§ˆì›Œìš©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                      Mention Title  \\\n",
       "0  360101-102055396016                                          Sulwhasoo   \n",
       "1  360101-141133562509  [ë‘ì½¤] ìˆ˜ì§€ê°€ ì†Œê°œí•˜ëŠ” ë¦¬ì–¼ ë©”ì´í¬ì—… íŒ! Suzy's Real Make-up t...   \n",
       "2   360101-92181315780                                                NaN   \n",
       "3  360101-133213819477  ê¹€ì„ ì•„ ê³µí•­íŒ¨ì…˜ ìŠ¤íƒ€ì¼ë§ ê¸€ì“´ì´ ë³´ë°° ë“±ë¡ì¼ 2019-11-04 15:24 ì¡°íšŒìˆ˜ ...   \n",
       "4   360101-74466121153                                                NaN   \n",
       "\n",
       "                                     Mention Content  \n",
       "0                          ì •ë‹µ : ì‰¬ì–´ ë˜ìŠ¤íŒ… ì´ì•„ë¦„ ë‹˜ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.  \n",
       "1                                            ì²œì‚¬ ìˆ˜ì§€â¤ï¸  \n",
       "2  #ì—ìŠ¤í‹°ë¡œë” 39ì»¬ëŸ¬ NEW ì‹ ìƒë¦½! #ì—”ë¹„ë¦½í˜ì¸íŠ¸ ì§€ì†ë ¥ì´ ì„ë§¤ë‚˜ ì§±ì§±í—Œì§€,,,,...  \n",
       "3  ì „ì²´ë³´ê¸° ì¼ìƒ ë¶€ë¶€ ì‹œì§‘/ì¹œì • ìœ¡ì•„ ì§ì¥ì‚´ë¦¼ ì¢‹ì€ê¸€ ì›ƒìŒ ê³ ë¯¼/ìµëª… ì—°ì˜ˆ/ì‹œì‚¬ ë‚´ê°€...  \n",
       "4                                 #ë‘ì½¤#ë¦½ìŠ¤í‹±#ë°•ì ¼ë‹˜ì„ ë¬¼ğŸŒºê³ ë§ˆì›Œìš©  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/my_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token\n",
    "from spacy.lang.ko import Korean\n",
    "from pipeline import ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set custom attributes\n",
    "# tabular ë°ì´í„°ì—ì„œ documentì˜ featureë“¤ì„ doc attributeë¡œ ë„˜ê¸¸ ìˆ˜ ìˆìŒ.\n",
    "Doc.set_extension('Id', default=None)\n",
    "Doc.set_extension('Mention', default=None)\n",
    "\n",
    "Token.set_extension('tag_', default=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wraaping custom tokenizer\n",
    "nlp = Korean()\n",
    "nlp.tokenizer = ko.SpacyTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context passingì„ í™œìš©í•˜ì.**\n",
    "    - dataframe $\\longrightarrow$ list of tuple (str, dict)\n",
    "    - ì—¬ê¸°ì„œ dictionaryì— context ì¦‰, ê° ë¬¸ì„œ(document)ì— ëŒ€ì‘í•˜ëŠ” featureë“¤ì„ contextí˜•íƒœë¡œ ë„˜ê¸°ì.\n",
    "    - ë³¸ ì˜ˆì œì—ì„œëŠ” 3000ê°œì˜ documentì— ëŒ€í•˜ì—¬ attributeë¡œ **idê°’**ê³¼ **titleì—¬ë¶€**ë¥¼ contextë¡œ ë„˜ê¸¸ ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('') # ê²°ì¸¡ê°’ pd.NA -> ''\n",
    "data = df.to_dict('r')\n",
    "mention_title = [(doc['Mention Title'], {'Id':doc['Id'], 'Mention':'Mention Title'}) \n",
    "                 for doc in data if (doc['Mention Title']!=None) and (doc['Mention Title'].strip() != '')]\n",
    "mention_content = [(doc['Mention Content'], {'Id':doc['Id'], 'Mention':'Mention Content'}) \n",
    "                   for doc in data if (doc['Mention Content']!=None) and (doc['Mention Content'].strip() != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114 1490\n"
     ]
    }
   ],
   "source": [
    "print(len(mention_title), len(mention_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **nlp.pipeì™€ doc_binì„ í†µí•´ íš¨ìœ¨ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì„±.**\n",
    "    - doc_binì˜ docê°ì²´ë¥¼ ì¢€ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "doc_bin = DocBin(store_user_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = mention_title + mention_content\n",
    "for doc, context in nlp.pipe(total, as_tuples=True, n_process=1): # process=1ì—ì„œ ì—ëŸ¬ê°€ ì—†ì„ ê²½ìš°, multiprocessë¡œ í™•ì¥.\n",
    "    doc._.Id = context['Id']\n",
    "    doc._.Mention = context['Mention']\n",
    "    doc_bin.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **get doc**\n",
    "    - __len__ì´ ì •ì˜ë˜ì–´ ìˆìŒ.\n",
    "    - `get_docs(nlp.vocab)`ì„ í†µí•´ docë“¤ì˜ generatorë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2604"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360101-102055396016 Mention Title\n",
      "sulwhasoo L "
     ]
    }
   ],
   "source": [
    "for doc in doc_bin.get_docs(nlp.vocab):\n",
    "    print(doc._.Id, doc._.Mention)\n",
    "    for token in doc:\n",
    "        print(token.text, token._.tag_, end=' ')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from `spacy` to `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention titleê³¼ mention contentë¥¼ ê°ê° dfë¡œ ë§Œë“¤ê¸°.\n",
    "doc_gen = doc_bin.get_docs(nlp.vocab)\n",
    "\n",
    "mention_title = []\n",
    "mention_content = []\n",
    "for doc in doc_gen:\n",
    "    if doc._.Mention == 'Mention Title':\n",
    "        mention_title.append({'Id':doc._.Id, doc._.Mention:doc.text})\n",
    "        continue\n",
    "    mention_content.append({'Id':doc._.Id, doc._.Mention:doc.text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- outer joinìœ¼ë¡œ í•©ì¹˜ë©´ ë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(mention_content).merge(pd.DataFrame(mention_title), how='outer')\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nlp`ì™€ `doc_bin`ì„ byteí™” í•œë‹¤.\n",
    "- `pickle`ë¡œ ì €ì¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nlp.pkl', 'wb') as f:\n",
    "    pickle.dump(nlp.to_bytes(), f)\n",
    "    \n",
    "with open('data/doc.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_bin.to_bytes(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# custom attributes you can access by _\n",
    "Doc.set_extension('Id', default=None)\n",
    "Doc.set_extension('Mention', default=None)\n",
    "\n",
    "Token.set_extension('tag_', default=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load nlp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('ko')\n",
    "with open('data/nlp.pkl', 'rb') as f:\n",
    "    nlp.from_bytes(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/doc.pkl', 'rb') as f:\n",
    "    doc_bin = DocBin(store_user_data=True).from_bytes(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212íƒ€ì„] ì„¤í™”ìˆ˜ ììŒ 2ì¢…ì„¸íŠ¸ 2018ë…„ (ì‡¼í•‘ë°± í¬í•¨)\n",
      "2604\n"
     ]
    }
   ],
   "source": [
    "print(docs[113])\n",
    "print(len(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
