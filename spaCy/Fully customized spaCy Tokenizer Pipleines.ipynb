{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully customized spaCy Tokenizer Pipleines\n",
    "- spaCy의 nlp에 대하여 가장 중요한 역할을 하는 것이 바로 tokenization이다.\n",
    "- spaCy는 규칙 기반의 tokenization 방식을 지원하지만, 기본적으로 라틴어에 적합한 형태임.\n",
    "- 그러므로, 아시아권 언어에 대해서는 각 국가별로 이미 만들어진 tokenizer를 활용하는 것이 좋음.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp pipeline에 토크나이저 wrapping하기.\n",
    "- spacy 한국어의 경우 mecab과 natto를 백본으로 만들어짐.\n",
    "- Domain-specific하게 학습한 soynlp tokenizer로 spacy tokenizer를 wrapping할 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 토크나이저 정의하기.\n",
    "#### callable object로 만들자.\n",
    "- input : text\n",
    "- output : Doc object\n",
    "\n",
    "#### `nlp.vocab`을 넘겨주자.\n",
    "- 언어별 기본 nlp를 넘겨주자.\n",
    "\n",
    "#### custom variable\n",
    "- 형태소 분석기반 토크나이저\n",
    "    - spacytokenizer 커스텀\n",
    "- 커스텀 품사 정의\n",
    "    - tokenizer 커스텀\n",
    "    - spacytokenizer 커스텀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시\n",
    "- soynlp 같은 경우 품사가 존재하지 않기 때문에 후처리로 L과 R을 처리하는 객체 정의함. (`pipeline/ko.py`확인)\n",
    "- SpacyTokenizer에 기 품사 또는 커스텀 품사를 attribute로 넘겨받기 위해서 Attribute extensions(`._.something`)을 활용함.\n",
    "    - 오피셜하게는 tag attribute에 wrapping하는 것이 맞는 방법이나, 간단하게 attribute extensions을 활용하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define custom attribute globally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension(\"tag_\", default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Define and initialize tokenizer and SpacyTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ko import Korean\n",
    "from pipeline import ko\n",
    "\n",
    "nlp = Korean()\n",
    "nlp.tokenizer = ko.SpacyTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요새 L\n",
      "코로나 L\n",
      "때문 L\n",
      "에 R\n",
      "마스크 L\n",
      "를 R\n",
      "많이 L\n",
      "쓰는 L\n",
      "사람들 L\n",
      "은 R\n",
      "피부 L\n",
      "가 R\n",
      "더 L\n",
      "거칠어지고 L\n",
      "건조 L\n",
      "하고 R\n",
      "자극 L\n",
      "을 R\n",
      "많이받아서 L\n",
      "피부 L\n",
      "트러블 L\n",
      "도 R\n",
      "많아 L\n",
      "지는 L\n",
      "시점 L\n",
      "이구요 R\n"
     ]
    }
   ],
   "source": [
    "for token in nlp('요새 코로나 때문에 마스크를 많이 쓰는 사람들은 피부가 더 거칠어지고 건조하고 자극을 많이받아서 피부 트러블도 많아 지는 시점이구요!'):\n",
    "    print(token.text, token._.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp pipline with text in tabular data\n",
    "- tabular data에 포함된 텍스트 데이터를 잘 분석하기 위해서는 데이터프레임과 spacy object 간의 자유로운 transform이 가능해야된다.\n",
    "- **df $\\longrightarrow$ spacy $\\longrightarrow$ df**와 같은 일련의 파이프라인을 구축하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from `df` to `spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tabular 데이터를 로드하고, spacy 변수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Mention Title</th>\n",
       "      <th>Mention Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360101-102055396016</td>\n",
       "      <td>Sulwhasoo</td>\n",
       "      <td>정답 : 쉬어 래스팅 이아름 님에게 알립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360101-141133562509</td>\n",
       "      <td>[랑콤] 수지가 소개하는 리얼 메이크업 팁! Suzy's Real Make-up t...</td>\n",
       "      <td>천사 수지❤️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360101-92181315780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#에스티로더 39컬러 NEW 신상립! #엔비립페인트 지속력이 을매나 짱짱헌지,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360101-133213819477</td>\n",
       "      <td>김선아 공항패션 스타일링 글쓴이 보배 등록일 2019-11-04 15:24 조회수 ...</td>\n",
       "      <td>전체보기 일상 부부 시집/친정 육아 직장살림 좋은글 웃음 고민/익명 연예/시사 내가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360101-74466121153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#랑콤#립스틱#박젼님선물🌺고마워용</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                      Mention Title  \\\n",
       "0  360101-102055396016                                          Sulwhasoo   \n",
       "1  360101-141133562509  [랑콤] 수지가 소개하는 리얼 메이크업 팁! Suzy's Real Make-up t...   \n",
       "2   360101-92181315780                                                NaN   \n",
       "3  360101-133213819477  김선아 공항패션 스타일링 글쓴이 보배 등록일 2019-11-04 15:24 조회수 ...   \n",
       "4   360101-74466121153                                                NaN   \n",
       "\n",
       "                                     Mention Content  \n",
       "0                          정답 : 쉬어 래스팅 이아름 님에게 알립니다.  \n",
       "1                                            천사 수지❤️  \n",
       "2  #에스티로더 39컬러 NEW 신상립! #엔비립페인트 지속력이 을매나 짱짱헌지,,,,...  \n",
       "3  전체보기 일상 부부 시집/친정 육아 직장살림 좋은글 웃음 고민/익명 연예/시사 내가...  \n",
       "4                                 #랑콤#립스틱#박젼님선물🌺고마워용  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/my_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token\n",
    "from spacy.lang.ko import Korean\n",
    "from pipeline import ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set custom attributes\n",
    "# tabular 데이터에서 document의 feature들을 doc attribute로 넘길 수 있음.\n",
    "Doc.set_extension('Id', default=None)\n",
    "Doc.set_extension('Mention', default=None)\n",
    "\n",
    "Token.set_extension('tag_', default=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wraaping custom tokenizer\n",
    "nlp = Korean()\n",
    "nlp.tokenizer = ko.SpacyTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Context passing을 활용하자.**\n",
    "    - dataframe $\\longrightarrow$ list of tuple (str, dict)\n",
    "    - 여기서 dictionary에 context 즉, 각 문서(document)에 대응하는 feature들을 context형태로 넘기자.\n",
    "    - 본 예제에서는 3000개의 document에 대하여 attribute로 **id값**과 **title여부**를 context로 넘길 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('') # 결측값 pd.NA -> ''\n",
    "data = df.to_dict('r')\n",
    "mention_title = [(doc['Mention Title'], {'Id':doc['Id'], 'Mention':'Mention Title'}) \n",
    "                 for doc in data if (doc['Mention Title']!=None) and (doc['Mention Title'].strip() != '')]\n",
    "mention_content = [(doc['Mention Content'], {'Id':doc['Id'], 'Mention':'Mention Content'}) \n",
    "                   for doc in data if (doc['Mention Content']!=None) and (doc['Mention Content'].strip() != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114 1490\n"
     ]
    }
   ],
   "source": [
    "print(len(mention_title), len(mention_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **nlp.pipe와 doc_bin을 통해 효율적인 파이프라인 구성.**\n",
    "    - doc_bin의 doc객체를 좀 더 메모리 효율적으로 관리함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "doc_bin = DocBin(store_user_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = mention_title + mention_content\n",
    "for doc, context in nlp.pipe(total, as_tuples=True, n_process=1): # process=1에서 에러가 없을 경우, multiprocess로 확장.\n",
    "    doc._.Id = context['Id']\n",
    "    doc._.Mention = context['Mention']\n",
    "    doc_bin.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **get doc**\n",
    "    - __len__이 정의되어 있음.\n",
    "    - `get_docs(nlp.vocab)`을 통해 doc들의 generator로 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2604"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360101-102055396016 Mention Title\n",
      "sulwhasoo L "
     ]
    }
   ],
   "source": [
    "for doc in doc_bin.get_docs(nlp.vocab):\n",
    "    print(doc._.Id, doc._.Mention)\n",
    "    for token in doc:\n",
    "        print(token.text, token._.tag_, end=' ')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from `spacy` to `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention title과 mention content를 각각 df로 만들기.\n",
    "doc_gen = doc_bin.get_docs(nlp.vocab)\n",
    "\n",
    "mention_title = []\n",
    "mention_content = []\n",
    "for doc in doc_gen:\n",
    "    if doc._.Mention == 'Mention Title':\n",
    "        mention_title.append({'Id':doc._.Id, doc._.Mention:doc.text})\n",
    "        continue\n",
    "    mention_content.append({'Id':doc._.Id, doc._.Mention:doc.text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- outer join으로 합치면 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(mention_content).merge(pd.DataFrame(mention_title), how='outer')\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nlp`와 `doc_bin`을 byte화 한다.\n",
    "- `pickle`로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nlp.pkl', 'wb') as f:\n",
    "    pickle.dump(nlp.to_bytes(), f)\n",
    "    \n",
    "with open('data/doc.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_bin.to_bytes(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# custom attributes you can access by _\n",
    "Doc.set_extension('Id', default=None)\n",
    "Doc.set_extension('Mention', default=None)\n",
    "\n",
    "Token.set_extension('tag_', default=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load nlp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('ko')\n",
    "with open('data/nlp.pkl', 'rb') as f:\n",
    "    nlp.from_bytes(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **load docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/doc.pkl', 'rb') as f:\n",
    "    doc_bin = DocBin(store_user_data=True).from_bytes(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212타임] 설화수 자음 2종세트 2018년 (쇼핑백 포함)\n",
      "2604\n"
     ]
    }
   ],
   "source": [
    "print(docs[113])\n",
    "print(len(docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
