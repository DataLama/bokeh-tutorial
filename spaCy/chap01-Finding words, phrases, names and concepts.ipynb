{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy의 핵심 기능 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `nlp` object\n",
    "<img src='https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg'>\n",
    "\n",
    "- nlp는 processing pipeline의 container 객체다.\n",
    "\n",
    "- 특이한건, nlp라는 이름의 클래스는 존재하지 않는다. 그냥 processing pipeline의 container를 nlp라는 변수에 주로 할당한다. (뭔 개소린가 싶겠지만, 우선 그러려니 하자.)\n",
    "\n",
    "- nlp 파이프라인에는 tokenizer, pos-tagger, dependency parser, ner, text categorizer와 같은 컴포넌트들이 있다.\n",
    "    - 영어의 경우 기본적으로 제공하는 컴포넌트가 많다. 예를 들면, ner 같은 것들... (한국어는 기대하지 말자.)\n",
    "    - 이러한 파이프라인의 컴포넌트들을 쉽게 customize할 수 있다. 예를 들면, bert + fine-tuning한 감정 분류 모델을 파이프라인에 추가할 수 있다.\n",
    "    \n",
    "    \n",
    "- nlp 파이프라인은 기본적으로 언어 별로 특화된 토크나이징 규칙을 갖고 있다.(요즘 대세인 subword tokenizer류 (bpe 등)들과 정반대의 철학을 갖고 있지만, 이 또한 나름대로 의미가 있다고 생각함.)\n",
    "    - `spacy.lang` 을 통해 접근 가능함.\n",
    "    - 기본적으로 지원하는 [언어 목록](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Korean language class\n",
    "from spacy.lang.ko import Korean\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Korean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc, Token and Span object (+ Lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://course.spacy.io/doc_span.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy의 자료구조는 기본적으로 `Doc`, `Token`, `Span`, `Lexeme`의 네 가지 객체를 사용한다.\n",
    "\n",
    "여기서 `Lexeme`은 문맥 독립적인(context independent) 특성을 갖고, `Doc`, `Token`, `Span`은 문맥 의존적(context dependent)한 특징을 갖는다.\n",
    "\n",
    "예를 들면, 어떤 토큰이 punct인지 아닌지에 대한 여부는 토큰 그 자체로 결정되지만, 토큰이 명사인지 아닌지는 문맥 의존적으로 결정되는 개념이다.\n",
    "\n",
    "이제 각각의 object에 대한 특징들을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc object\n",
    "- 문자열 형태의 텍스트 데이터가 `nlp` 를 통과하면 `doc`을 반환함.\n",
    "- `doc`은 document의 약자다.\n",
    "- `doc`은 텍스트 데이터의 다양한 층위의 정보들을 구조적으로 접근할 수 있게 해준다.\n",
    "- `doc`은 어떠한 정보 손실이 없다.\n",
    "- `doc`은 iterable하고, index로 token에 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp('지금 시각은 밤 9시 41분 집에 가고 싶네요.')\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금\n",
      "시각\n",
      "은\n",
      "밤\n",
      "9\n",
      "시\n",
      "41\n",
      "분\n",
      "집\n",
      "에\n",
      "가\n",
      "고\n",
      "싶\n",
      "네요\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "분"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing Doc object\n",
    "doc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token object\n",
    "\n",
    "- `Token`은 문서 내 토큰에 해당하는 객체들을 표현한다.\n",
    "    - 단어, punctuation 문자 등등을 표현할 수 있다.\n",
    "    \n",
    "    \n",
    "- `Token`은 토큰에 대한 정보를 다양한 attribute를 통해서 접근할 수 있도록 해준다.\n",
    "    - 예를 들면, 토큰에 해당하는 literal한 텍스트는 `token.text`를 통해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세상\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"안녕 세상, Hello world!\")\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span object\n",
    "- `Span`은 `doc`을 하나 이상의 token들의 \n",
    "    - `doc`에 대한 `token`들의 연속적인 subset으로 생각하면 됨.\n",
    "\n",
    "\n",
    "- `Span`은 `doc`에 대한 view이고, 실제 데이터를 갖고 있지는 않다.\n",
    "\n",
    "\n",
    "- `doc`을 slicing하면 span을 얻을 수 있다.\n",
    "    - 여기서 slicing은 `[, )`방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "세상, Hello\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"안녕 세상, Hello world!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Attributes\n",
    "- **token attributes**\n",
    "    - `i`\n",
    "        - `doc`에서 몇 번째 위치에 해당 토큰이 위치하는가?\n",
    "    - `text`\n",
    "        - 토큰의 literal한 텍스트를 반환함.\n",
    "    \n",
    "- **lexical attributes**\n",
    "    - `is_alpha`\n",
    "        - 토큰이 문자(unicode level)인가?\n",
    "    - `is_punct`\n",
    "        - 해당 토큰이 punctuation인가?\n",
    "    - `like_num`\n",
    "        - 해당 토큰이 숫자인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Text:     ['오늘', '🤗', 'transformers', '의', 'star', '갯', '수', '는', '24184', '다', '!', '!']\n",
      "is_alpha: [True, False, True, True, True, True, True, True, False, True, False, False]\n",
      "is_punct: [False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "like_num: [False, False, False, False, False, False, False, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"오늘 🤗 transformers의 star 갯수는 24184다!!\") \n",
    "## token attributes\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "## lexical attributes\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistical models를 한 마디로 정리하면, **linguistic feature를 뽑을 수 있는 모델들의 파이프라인**이다.\n",
    "\n",
    "spaCy은 다양한 statistical models들을 제공하지만, 실제 내가 원하는 NLP 파이프라인을 만들기 위해서는 커스터마이징이 필수적이다.\n",
    "\n",
    "혹자는 그러면 왜 굳이 spaCy를 쓰냐고 할 수 있지만, spaCy의 장점은 텍스트에서 정량화해야될 linguistic feature가 많아질수록, 서비스를 제공해야될 언어의 수가 많아질수록 빛을 발한다.\n",
    "\n",
    "예를 들면, 텍스트 데이터에 대해서 다음과 같은 정량화된 feature를 추출해야되는 상황이 있다고 가정해보자.\n",
    "\n",
    "```\n",
    "- Basic\n",
    "    - tokenization\n",
    "\n",
    "- token-level linguistic features\n",
    "    - pos-tagging\n",
    "    - keyword extraction\n",
    "    - tf-idf\n",
    "    - word vector\n",
    "    - ner\n",
    "    - text-network analysis\n",
    "    - ...\n",
    "    \n",
    "- document-level linguistic features\n",
    "    - topic\n",
    "    - document vector\n",
    "    - sentiment\n",
    "    - ...\n",
    "\n",
    "- available langs : ko, jp, zh, en\n",
    "```\n",
    "\n",
    "대충 생각해봐도 수십개의 nlp ML/DL 모델들이 필요하다. \n",
    "\n",
    "이러한 모델들을 효율적으로 관리하여 좋은 NLP 서비스를 개발할 수 있도록 도와주는 역할을 하는 것이 바로 spacy다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy는 기본적으로 [수 많은 pre-trained된 모델 패키지](https://spacy.io/models)들을 제공한다.\n",
    "    - 물론 한국어는 없다.\n",
    "- `en_core_web_sm` 패키지 예시\n",
    "    - 영어 웹 데이터(blogs, news, comments)\n",
    "    - pipeline 기능 pos-tagger, parser, ner\n",
    "    - `!python -m spacy download en_core_web_sm`\n",
    "- nlp object를 통해 처리가 될 때, 모델들의 inference가 발생한다.\n",
    "- 기타 특징.\n",
    "    - binary weights\n",
    "    - vocabulary\n",
    "    - meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model packages\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a text\n",
    "doc = nlp('Microsoft is the IT company.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pos-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft PROPN\n",
      "is AUX\n",
      "the DET\n",
      "IT PROPN\n",
      "company NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft PROPN nsubj is\n",
      "is AUX ROOT is\n",
      "the DET det company\n",
      "IT PROPN compound company\n",
      "company NOUN attr is\n",
      ". PUNCT punct is\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이즈가 잘못되었을 경우 NER의 결과가 제대로 안나올 수 있다.(e.g iPhone X) 어떻게 해결할까???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not just regular expressions?\n",
    "- 정규표현식은 text의 형태적인 특징만 판단하지만, spacy rule-based matching은 text의 형태적인 특징뿐만 아니라 token에 대한 다양한 Linguistic feature들을 복합적으로 고려할 수 있다.\n",
    "- 물론 이런 추출 방식이 유의하려면 Linguistic feature를 tagging하는 모델들의 성능이 보장되어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match patterns이란\n",
    "- spacy에서 pattern matching을 위해 사용되는 조건들임.\n",
    "- list of dictionary형태로 각 dictionary가 하나의 token에 매칭된다.\n",
    "- dictionary의 key는 token의 attributes고, value는 그 attributes에서 기대되는 값과 매칭된다.\n",
    "    - key에 해당하는 값은 대문자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Match exact token texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Match lexical attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Match any token attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher를 활용한 pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matcher를 로드하고 nlp.vocab으로 initialize하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list of dictionary 형태로 패턴을 정의하고, matcher에 추가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pattern to the matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern) # 두번째 attribute는 callback 기능이라는데... 우선... 무시.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 패턴 매칭을 수행할 document를 넘겨주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `match_id`: hash value of the pattern name\n",
    "- `start`: start index of matched span\n",
    "- `end`: end index of matched span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use match results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More application of Matcher\n",
    "- get my matching pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, [[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.get('IPHONE_PATTERN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove pattern by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('IPHONE_PATTERN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matching complex pattern with token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "matcher.add('LOVE_SOMETHING', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matcher(doc):\n",
    "    matcher_span = doc[start: end]\n",
    "    print(matcher_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using operators and quantifiers.\n",
    "    - `OP`라는 키를 활용해서 패턴의 갯수를 지정할 수 있다.\n",
    "    - `{'OP':'!'}` Negation: match 0 times.\n",
    "    - `{'OP':'?'}` Optional: match 0 or 1 times.\n",
    "    - `{'OP':'+'}` Match 1 or more times.\n",
    "    - `{'OP':'*'}` Match 0 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "matcher.add('BUY_SOMETHING', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matcher(doc):\n",
    "    matcher_span = doc[start: end]\n",
    "    print(matcher_span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
