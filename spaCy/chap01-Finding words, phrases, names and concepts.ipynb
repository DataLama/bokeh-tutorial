{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCyì˜ í•µì‹¬ ê¸°ëŠ¥ ì†Œê°œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `nlp` object\n",
    "<img src='https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg'>\n",
    "\n",
    "- nlpëŠ” processing pipelineì˜ container ê°ì²´ë‹¤.\n",
    "\n",
    "- íŠ¹ì´í•œê±´, nlpë¼ëŠ” ì´ë¦„ì˜ í´ë˜ìŠ¤ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëƒ¥ processing pipelineì˜ containerë¥¼ nlpë¼ëŠ” ë³€ìˆ˜ì— ì£¼ë¡œ í• ë‹¹í•œë‹¤. (ë­” ê°œì†Œë¦°ê°€ ì‹¶ê² ì§€ë§Œ, ìš°ì„  ê·¸ëŸ¬ë ¤ë‹ˆ í•˜ì.)\n",
    "\n",
    "- nlp íŒŒì´í”„ë¼ì¸ì—ëŠ” tokenizer, pos-tagger, dependency parser, ner, text categorizerì™€ ê°™ì€ ì»´í¬ë„ŒíŠ¸ë“¤ì´ ìˆë‹¤.\n",
    "    - ì˜ì–´ì˜ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ì»´í¬ë„ŒíŠ¸ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ner ê°™ì€ ê²ƒë“¤... (í•œêµ­ì–´ëŠ” ê¸°ëŒ€í•˜ì§€ ë§ì.)\n",
    "    - ì´ëŸ¬í•œ íŒŒì´í”„ë¼ì¸ì˜ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì‰½ê²Œ customizeí•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, bert + fine-tuningí•œ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì„ íŒŒì´í”„ë¼ì¸ì— ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.\n",
    "    \n",
    "    \n",
    "- nlp íŒŒì´í”„ë¼ì¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì–¸ì–´ ë³„ë¡œ íŠ¹í™”ëœ í† í¬ë‚˜ì´ì§• ê·œì¹™ì„ ê°–ê³  ìˆë‹¤.(ìš”ì¦˜ ëŒ€ì„¸ì¸ subword tokenizerë¥˜ (bpe ë“±)ë“¤ê³¼ ì •ë°˜ëŒ€ì˜ ì² í•™ì„ ê°–ê³  ìˆì§€ë§Œ, ì´ ë˜í•œ ë‚˜ë¦„ëŒ€ë¡œ ì˜ë¯¸ê°€ ìˆë‹¤ê³  ìƒê°í•¨.)\n",
    "    - `spacy.lang` ì„ í†µí•´ ì ‘ê·¼ ê°€ëŠ¥í•¨.\n",
    "    - ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” [ì–¸ì–´ ëª©ë¡](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Korean language class\n",
    "from spacy.lang.ko import Korean\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = Korean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc, Token and Span object (+ Lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://course.spacy.io/doc_span.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacyì˜ ìë£Œêµ¬ì¡°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `Doc`, `Token`, `Span`, `Lexeme`ì˜ ë„¤ ê°€ì§€ ê°ì²´ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œ `Lexeme`ì€ ë¬¸ë§¥ ë…ë¦½ì ì¸(context independent) íŠ¹ì„±ì„ ê°–ê³ , `Doc`, `Token`, `Span`ì€ ë¬¸ë§¥ ì˜ì¡´ì (context dependent)í•œ íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´, ì–´ë–¤ í† í°ì´ punctì¸ì§€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì—¬ë¶€ëŠ” í† í° ê·¸ ìì²´ë¡œ ê²°ì •ë˜ì§€ë§Œ, í† í°ì´ ëª…ì‚¬ì¸ì§€ ì•„ë‹Œì§€ëŠ” ë¬¸ë§¥ ì˜ì¡´ì ìœ¼ë¡œ ê²°ì •ë˜ëŠ” ê°œë…ì´ë‹¤.\n",
    "\n",
    "ì´ì œ ê°ê°ì˜ objectì— ëŒ€í•œ íŠ¹ì§•ë“¤ì„ ì‚´í´ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc object\n",
    "- ë¬¸ìì—´ í˜•íƒœì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ `nlp` ë¥¼ í†µê³¼í•˜ë©´ `doc`ì„ ë°˜í™˜í•¨.\n",
    "- `doc`ì€ documentì˜ ì•½ìë‹¤.\n",
    "- `doc`ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ì¸µìœ„ì˜ ì •ë³´ë“¤ì„ êµ¬ì¡°ì ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.\n",
    "- `doc`ì€ ì–´ë– í•œ ì •ë³´ ì†ì‹¤ì´ ì—†ë‹¤.\n",
    "- `doc`ì€ iterableí•˜ê³ , indexë¡œ tokenì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp('ì§€ê¸ˆ ì‹œê°ì€ ë°¤ 9ì‹œ 41ë¶„ ì§‘ì— ê°€ê³  ì‹¶ë„¤ìš”.')\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§€ê¸ˆ\n",
      "ì‹œê°\n",
      "ì€\n",
      "ë°¤\n",
      "9\n",
      "ì‹œ\n",
      "41\n",
      "ë¶„\n",
      "ì§‘\n",
      "ì—\n",
      "ê°€\n",
      "ê³ \n",
      "ì‹¶\n",
      "ë„¤ìš”\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ë¶„"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing Doc object\n",
    "doc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token object\n",
    "\n",
    "- `Token`ì€ ë¬¸ì„œ ë‚´ í† í°ì— í•´ë‹¹í•˜ëŠ” ê°ì²´ë“¤ì„ í‘œí˜„í•œë‹¤.\n",
    "    - ë‹¨ì–´, punctuation ë¬¸ì ë“±ë“±ì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n",
    "    \n",
    "    \n",
    "- `Token`ì€ í† í°ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹¤ì–‘í•œ attributeë¥¼ í†µí•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤.\n",
    "    - ì˜ˆë¥¼ ë“¤ë©´, í† í°ì— í•´ë‹¹í•˜ëŠ” literalí•œ í…ìŠ¤íŠ¸ëŠ” `token.text`ë¥¼ í†µí•´ ë³¼ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¸ìƒ\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"ì•ˆë…• ì„¸ìƒ, Hello world!\")\n",
    "\n",
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span object\n",
    "- `Span`ì€ `doc`ì„ í•˜ë‚˜ ì´ìƒì˜ tokenë“¤ì˜ \n",
    "    - `doc`ì— ëŒ€í•œ `token`ë“¤ì˜ ì—°ì†ì ì¸ subsetìœ¼ë¡œ ìƒê°í•˜ë©´ ë¨.\n",
    "\n",
    "\n",
    "- `Span`ì€ `doc`ì— ëŒ€í•œ viewì´ê³ , ì‹¤ì œ ë°ì´í„°ë¥¼ ê°–ê³  ìˆì§€ëŠ” ì•Šë‹¤.\n",
    "\n",
    "\n",
    "- `doc`ì„ slicingí•˜ë©´ spanì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "    - ì—¬ê¸°ì„œ slicingì€ `[, )`ë°©ì‹ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¸ìƒ, Hello\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"ì•ˆë…• ì„¸ìƒ, Hello world!\")\n",
    "\n",
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Attributes\n",
    "- **token attributes**\n",
    "    - `i`\n",
    "        - `doc`ì—ì„œ ëª‡ ë²ˆì§¸ ìœ„ì¹˜ì— í•´ë‹¹ í† í°ì´ ìœ„ì¹˜í•˜ëŠ”ê°€?\n",
    "    - `text`\n",
    "        - í† í°ì˜ literalí•œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•¨.\n",
    "    \n",
    "- **lexical attributes**\n",
    "    - `is_alpha`\n",
    "        - í† í°ì´ ë¬¸ì(unicode level)ì¸ê°€?\n",
    "    - `is_punct`\n",
    "        - í•´ë‹¹ í† í°ì´ punctuationì¸ê°€?\n",
    "    - `like_num`\n",
    "        - í•´ë‹¹ í† í°ì´ ìˆ«ìì¸ê°€?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Text:     ['ì˜¤ëŠ˜', 'ğŸ¤—', 'transformers', 'ì˜', 'star', 'ê°¯', 'ìˆ˜', 'ëŠ”', '24184', 'ë‹¤', '!', '!']\n",
      "is_alpha: [True, False, True, True, True, True, True, True, False, True, False, False]\n",
      "is_punct: [False, False, False, False, False, False, False, False, False, False, True, True]\n",
      "like_num: [False, False, False, False, False, False, False, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"ì˜¤ëŠ˜ ğŸ¤— transformersì˜ star ê°¯ìˆ˜ëŠ” 24184ë‹¤!!\") \n",
    "## token attributes\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "## lexical attributes\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistical modelsë¥¼ í•œ ë§ˆë””ë¡œ ì •ë¦¬í•˜ë©´, **linguistic featureë¥¼ ë½‘ì„ ìˆ˜ ìˆëŠ” ëª¨ë¸ë“¤ì˜ íŒŒì´í”„ë¼ì¸**ì´ë‹¤.\n",
    "\n",
    "spaCyì€ ë‹¤ì–‘í•œ statistical modelsë“¤ì„ ì œê³µí•˜ì§€ë§Œ, ì‹¤ì œ ë‚´ê°€ ì›í•˜ëŠ” NLP íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ í•„ìˆ˜ì ì´ë‹¤.\n",
    "\n",
    "í˜¹ìëŠ” ê·¸ëŸ¬ë©´ ì™œ êµ³ì´ spaCyë¥¼ ì“°ëƒê³  í•  ìˆ˜ ìˆì§€ë§Œ, spaCyì˜ ì¥ì ì€ í…ìŠ¤íŠ¸ì—ì„œ ì •ëŸ‰í™”í•´ì•¼ë  linguistic featureê°€ ë§ì•„ì§ˆìˆ˜ë¡, ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•´ì•¼ë  ì–¸ì–´ì˜ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë¹›ì„ ë°œí•œë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ë©´, í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì •ëŸ‰í™”ëœ featureë¥¼ ì¶”ì¶œí•´ì•¼ë˜ëŠ” ìƒí™©ì´ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.\n",
    "\n",
    "```\n",
    "- Basic\n",
    "    - tokenization\n",
    "\n",
    "- token-level linguistic features\n",
    "    - pos-tagging\n",
    "    - keyword extraction\n",
    "    - tf-idf\n",
    "    - word vector\n",
    "    - ner\n",
    "    - text-network analysis\n",
    "    - ...\n",
    "    \n",
    "- document-level linguistic features\n",
    "    - topic\n",
    "    - document vector\n",
    "    - sentiment\n",
    "    - ...\n",
    "\n",
    "- available langs : ko, jp, zh, en\n",
    "```\n",
    "\n",
    "ëŒ€ì¶© ìƒê°í•´ë´ë„ ìˆ˜ì‹­ê°œì˜ nlp ML/DL ëª¨ë¸ë“¤ì´ í•„ìš”í•˜ë‹¤. \n",
    "\n",
    "ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ ì¢‹ì€ NLP ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ spacyë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ [ìˆ˜ ë§ì€ pre-trainedëœ ëª¨ë¸ íŒ¨í‚¤ì§€](https://spacy.io/models)ë“¤ì„ ì œê³µí•œë‹¤.\n",
    "    - ë¬¼ë¡  í•œêµ­ì–´ëŠ” ì—†ë‹¤.\n",
    "- `en_core_web_sm` íŒ¨í‚¤ì§€ ì˜ˆì‹œ\n",
    "    - ì˜ì–´ ì›¹ ë°ì´í„°(blogs, news, comments)\n",
    "    - pipeline ê¸°ëŠ¥ pos-tagger, parser, ner\n",
    "    - `!python -m spacy download en_core_web_sm`\n",
    "- nlp objectë¥¼ í†µí•´ ì²˜ë¦¬ê°€ ë  ë•Œ, ëª¨ë¸ë“¤ì˜ inferenceê°€ ë°œìƒí•œë‹¤.\n",
    "- ê¸°íƒ€ íŠ¹ì§•.\n",
    "    - binary weights\n",
    "    - vocabulary\n",
    "    - meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model packages\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a text\n",
    "doc = nlp('Microsoft is the IT company.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pos-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft PROPN\n",
      "is AUX\n",
      "the DET\n",
      "IT PROPN\n",
      "company NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft PROPN nsubj is\n",
      "is AUX ROOT is\n",
      "the DET det company\n",
      "IT PROPN compound company\n",
      "company NOUN attr is\n",
      ". PUNCT punct is\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft ORG\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í† í¬ë‚˜ì´ì¦ˆê°€ ì˜ëª»ë˜ì—ˆì„ ê²½ìš° NERì˜ ê²°ê³¼ê°€ ì œëŒ€ë¡œ ì•ˆë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.(e.g iPhone X) ì–´ë–»ê²Œ í•´ê²°í• ê¹Œ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
